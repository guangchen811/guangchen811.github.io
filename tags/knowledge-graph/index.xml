<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Knowledge Graph on Hao Chen</title>
    <link>https://guangchen811.github.io/tags/knowledge-graph/</link>
    <description>Recent content in Knowledge Graph on Hao Chen</description>
    <image>
      <title>Hao Chen</title>
      <url>https://guangchen811.github.io/papermod-cover.png</url>
      <link>https://guangchen811.github.io/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 21 May 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://guangchen811.github.io/tags/knowledge-graph/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Language &#43; Knowledge: The Mutual Reinforcement of Large Language Models and Knowledge Graphs</title>
      <link>https://guangchen811.github.io/llm/kg_and_llm/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      <guid>https://guangchen811.github.io/llm/kg_and_llm/</guid>
      <description>Large language models (LLMs) have shown great success in many NLP tasks, including question answering, text generation, and more. However, the accuracy, consistency, and explainability of LLMs still hinder their application in many real-world scenarios. To address these problems, researchers have proposed numerous methods to enhance these abilities in LLMs. One of the most promising methods involves combining LLMs with knowledge graphs (KGs), a high-quality, structured knowledge source known for its reasonable explainability.</description>
    </item>
  </channel>
</rss>
