<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>knowledge graph on Hao&#39;Log</title>
    <link>https://guangchen811.github.io/tags/knowledge-graph/</link>
    <description>Recent content in knowledge graph on Hao&#39;Log</description>
    <image>
      <url>https://guangchen811.github.io/papermod-cover.png</url>
      <link>https://guangchen811.github.io/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 23 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://guangchen811.github.io/tags/knowledge-graph/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[DRAFT]: Language &#43; Knowledge: The Mutual Reinforcement of Large Language Models and Knowledge Graphs</title>
      <link>https://guangchen811.github.io/llm/kg_and_llm/</link>
      <pubDate>Sun, 23 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://guangchen811.github.io/llm/kg_and_llm/</guid>
      <description>Large language models (LLMs) has show great success in many NLP tasks, including question answering, text generation, and so on. However, the accuaracy, consistency and explainability of LLMs still block the application of LLMs in many real-world scenarios. To address these problems, researchers have proposed many methods to improve the mentioned abilities of LLMs. One of the most promising methods is to combine LLMs with knowledge graphs (KGs), which is a high-quality structured knowledge source with reasonable explainability.</description>
    </item>
    
  </channel>
</rss>
